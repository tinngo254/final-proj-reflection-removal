{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-1-1fc0462f9f26>:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import pathlib\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TENSORFLOW 2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print('GPU available:', tf.test.is_gpu_available())\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Add, BatchNormalization, Conv2D, Dense, Flatten, Input, LeakyReLU, PReLU, Lambda, MaxPool2D\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError, MeanSquaredError\n",
    "binary_cross_entropy = BinaryCrossentropy()\n",
    "\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "# import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "\n",
    "# ESSENTIAL \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "# VISUALIZER\n",
    "import matplotlib.pyplot as plt\n",
    "import mahotas \n",
    "import imutils\n",
    "plt.ioff()\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# UTILS\n",
    "import time\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(27)\n",
    "np.random.seed(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained VGG \n",
    "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
    "vgg_19 = VGG19(input_shape=(None, None, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8d89083d995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DIRECTORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "GAN\n",
    "│   \n",
    "└───data\n",
    "│   │\n",
    "│   └───real_images\n",
    "│   │    │  \n",
    "│   │    └─── tranmission\n",
    "│   │    │  \n",
    "│   │    └─── blended\n",
    "│   │\n",
    "│   └───synthetic_images\n",
    "│        │  \n",
    "│        └─── tranmission\n",
    "│        │  \n",
    "│        └─── reflection\n",
    "│   \n",
    "└───logs\n",
    "    │\n",
    "    └─── ckpts \n",
    "    │    │  \n",
    "    │    └─── pretrain \n",
    "    │    │  \n",
    "    │    └─── train \n",
    "    │\n",
    "    └─── output\n",
    "         │  \n",
    "         └─── pretrain \n",
    "         │  \n",
    "         └─── train \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Directories\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "BLENDED_TRAIN_DIR = os.path.join(DATA_DIR, 'real_images/blended')\n",
    "TRANSMISSION_TRAIN_DIR = os.path.join(DATA_DIR, 'real_images/tranmission')\n",
    "\n",
    "SYN_REFLECTION_TRAIN_DIR = os.path.join(DATA_DIR, 'synthetic_images/reflection')\n",
    "SYN_TRANSMISSION_TRAIN_DIR = os.path.join(DATA_DIR, 'synthetic_images/tranmission')\n",
    "\n",
    "BLENDED_TEST_DIR = os.path.join()\n",
    "TRANSMISSION_TEST_DIR = os.path.join()\n",
    "\n",
    "# IF THERE ARE SAMPLES\n",
    "SAMPLE_DIR = os.path.join()\n",
    "\n",
    "# Training Support Directories \n",
    "LOG_DIR = './logs'\n",
    "\n",
    "CKPT_DIR  = os.path.join(LOG_DIR, 'ckpts')\n",
    "OUTPUT_DIR  = os.path.join(LOG_DIR, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image Configuration\n",
    "# SCALE = 4  \n",
    "# HR_SIZE = 128\n",
    "# LR_SIZE = HR_SIZE//SCALE \n",
    "# CHANNEL = 3\n",
    "BUFFER_SIZE = 400\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 400\n",
    "PRETRAIN_LR = 2e-4\n",
    "PRETRAIN_LR_DECAY_STEP = 20000\n",
    "GAN_LR = 1e-4\n",
    "\n",
    "# Loss Configuration \n",
    "GAN_LOSS_COEFF = 0.005\n",
    "CONTENT_LOSS_COEFF = 0.01\n",
    "\n",
    "EPS = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please remember to use tf.Dataset & tf.image.ImageGenerator to load images to tf dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    \"\"\" Class to call a pipeline that load images from paths to Tensorflow dataset\n",
    "    \"\"\"\n",
    "    def load_paths_from_directory(self, directory):\n",
    "        return sorted(glob(os.path.join(directory, \"*.{}\".format('jpg'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocess(object):\n",
    "    \"\"\" Preprocess images before flowing into the model.\n",
    "        Perform random crop into training size, random augmentation\n",
    "\n",
    "    \"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    \n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                  method= tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                                 method= tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, \n",
    "                                         size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "  \n",
    "    return cropped_image[0], cropped_image[1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "  \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    # resizing to 512 x 512 x 3\n",
    "    input_image, real_image = resize(input_image, real_image, 512, 512)\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "  \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,\n",
    "                                     IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "  \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOME TEST IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "trans_test = cv2.imread('./Test-Images/back_2.jpg', -1)\n",
    "blend_test = cv2.imread('./Test-Images/blended_2.jpg', -1)\n",
    "\n",
    "trans_image = tf.io.read_file('./Test-Images/back_2.jpg')\n",
    "trans_image = tf.image.decode_jpeg(trans_image, channels= 3)\n",
    "\n",
    "##########\n",
    "neww = 128\n",
    "newh = 128\n",
    "\n",
    "channel = 64\n",
    "\n",
    "########## RESIZE\n",
    "output_t = cv2.resize(np.float32(trans_test), (neww,newh), cv2.INTER_CUBIC)/255.0\n",
    "output_b = cv2.resize(np.float32(blend_test), (neww,newh), cv2.INTER_CUBIC)/255.0\n",
    "\n",
    "output_image = tf.image.resize(trans_image, [neww, newh],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "######### EXPAND DIM\n",
    "output_t = np.expand_dims(output_t,axis=0)\n",
    "output_b = np.expand_dims(output_b,axis=0)\n",
    "\n",
    "output_image = tf.expand_dims(output_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (1, 128, 128, 64)\n",
      "1 (1, 128, 128, 64)\n",
      "1 (1, 128, 128, 67)\n",
      "2 (1, 64, 64, 128)\n",
      "2 (1, 128, 128, 128)\n",
      "2 (1, 128, 128, 195)\n",
      "3 (1, 32, 32, 256)\n",
      "3 (1, 128, 128, 256)\n",
      "3 (1, 128, 128, 451)\n",
      "4 (1, 16, 16, 512)\n",
      "4 (1, 128, 128, 512)\n",
      "4 (1, 128, 128, 963)\n",
      "5 (1, 8, 8, 512)\n",
      "5 (1, 128, 128, 512)\n",
      "5 (1, 128, 128, 1475)\n"
     ]
    }
   ],
   "source": [
    "# input_img = output_t #output_image   #output_t\n",
    "# in_image = output_t #output_image  #output_t\n",
    "\n",
    "# for block_id in range(1, 6):\n",
    "#     weight = vgg_weight_block(input_img, block_id)\n",
    "#     print(block_id, weight.shape)\n",
    "#     weight_resize = tf.image.resize(weight, (tf.shape(input= input_img)[1], tf.shape(input= input_img)[2]), \n",
    "#                                            method = tf.image.ResizeMethod.BILINEAR) / 255.0\n",
    "#     print(block_id, weight_resize.shape)\n",
    "    \n",
    "    \n",
    "# #     in_img = tf.concat([tf.image.resize(weight, (tf.shape(input= in_img)[1], tf.shape(input= in_img)[2]), \n",
    "# #                                            method=tf.image.ResizeMethod.BILINEAR) / 255.0, \n",
    "# #                           in_img], axis=3)\n",
    "#     in_image = tf.concat([weight_resize, in_image], axis= 3)\n",
    "#     print(block_id, in_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(ds):\n",
    "    i = 0\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for lr in ds.take(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.imshow((lr.numpy()[0] * 255).astype('int'))\n",
    "        plt.axis('off')\n",
    "        i+=1\n",
    "        plt.title('sample_{}'.format(i))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def visualize_ds(ds):\n",
    "    for lr, hr in ds.take(1):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        count_plot=1\n",
    "        for i in range(3):\n",
    "            plt.subplot(3, 2, count_plot)\n",
    "            plt.imshow((lr.numpy()[i] * 255).astype('int'))\n",
    "            plt.axis('off')\n",
    "            plt.title('LR')\n",
    "\n",
    "            plt.subplot(3, 2, count_plot+1)\n",
    "            plt.imshow((hr.numpy()[i] * 255).astype('int'))\n",
    "            plt.title('HR')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            count_plot+=2\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_and_save_images(model, epoch, sample_ds, train_times=1, mode='train'):\n",
    "#     i = 0\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     for sample in sample_ds.take(4):\n",
    "#         i+=1\n",
    "#         predictions = model(sample, training=False)\n",
    "#         predictions = ((predictions.numpy()[0] * 255).astype('int'))\n",
    "#         plt.subplot(2, 2, i)\n",
    "#         plt.imshow(predictions)\n",
    "#         plt.axis('off')\n",
    "    \n",
    "#     path = os.path.join(OUTPUT_DIR, mode, 'img_at_{}_epoch_{:04d}.png'.format(train_times, epoch)) \n",
    "#     plt.savefig(path)\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "  \n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "  \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some introduction about the model, blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vgg(output_layer):\n",
    "    \"\"\" Get VGG19 layers as a model, from input layer\n",
    "        to the chosen ouput layers\n",
    "    \"\"\"\n",
    "    return Model(vgg_19.input, vgg_19.layers[output_layer].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 blocks vggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg_mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "# def get_vgg_blocks():\n",
    "#     # All required VGG19 layers for this model\n",
    "#     block_vgg_0 = _vgg(0)   # Input, layer 0 \n",
    "#     block_vgg_1 = _vgg(2)   # conv1_2, layer 2 \n",
    "#     block_vgg_2 = _vgg(5)   # conv2_2, layer 5 \n",
    "#     block_vgg_3 = _vgg(8)   # conv3_2, layer 8 \n",
    "#     block_vgg_4 = _vgg(13)  # conv4_2, layer 13\n",
    "#     block_vgg_5 = _vgg(18)  # conv5_2, layer 18\n",
    "\n",
    "#     block_vgg_list = []\n",
    "#     for i in range(0, 6):\n",
    "#         block_vgg_list.append( eval('block_vgg_{}'.format(i)) )\n",
    "#     return block_vgg_list\n",
    "\n",
    "# def preprocess_vgg(in_img):\n",
    "#     in_img = tf.cast(in_img * 255., dtype=tf.float32)\n",
    "#     r, g, b = tf.split(in_img, 3, 3)\n",
    "#     bgr = tf.concat([b - vgg_mean[0],\n",
    "#                      g - vgg_mean[1],\n",
    "#                      r - vgg_mean[2]], axis=3)\n",
    "#     return bgr\n",
    "\n",
    "# block_vgg_list = get_vgg_blocks()\n",
    "\n",
    "# def vgg_weight_block(in_img, block_num):\n",
    "#     \"\"\" Get weights from blocks of VGG19 pre-trained model\n",
    "#         Following research paper, get the hypercolumn feature of layer 'conv_(1,2,3,4,5)_2'\n",
    "#         and concatenate to input before entering Generator's layers\n",
    "#     \"\"\"\n",
    "#     img = preprocess_vgg(in_img)\n",
    "#     return block_vgg_list[block_num](img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg layers from input to conv5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VGG Weights\n",
    "# vgg_mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "# def vgg_52(input):\n",
    "#     \"\"\" Get weights from VGG19 pre-trained model\n",
    "#         Following research paper, get the hypercolumn feature until layer 'conv_5_2'\n",
    "#     \"\"\"\n",
    "#     input = tf.cast(input * 255., dtype=tf.float32)\n",
    "#     r, g, b = tf.split(input, 3, 3)\n",
    "#     bgr = tf.concat([b - vgg_mean[0],\n",
    "#                      g - vgg_mean[1],\n",
    "#                      r - vgg_mean[2]], axis=3)\n",
    "#     return _vgg(18)(bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images with VGG means\n",
    "vgg_mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "def preprocess_vgg_means(input_img):\n",
    "    \"\"\" Subtract vgg means from images\n",
    "    \"\"\"\n",
    "#     input_img = tf.cast(input_img * 255., dtype= tf.float32)\n",
    "    input_img = tf.cast(input_img * 255, dtype= tf.float32) # If tf.readfile -> img will be int type, for numpy, I've already converted above\n",
    "    r, g, b = tf.split(input_img, 3, 3)\n",
    "    bgr = tf.concat([b - vgg_mean[0],\n",
    "                     g - vgg_mean[1],\n",
    "                     r - vgg_mean[2]], axis=3)\n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VGG Weights\n",
    "\n",
    "def get_vgg_weights(input_img):\n",
    "    \"\"\" Get weights from VGG19 pre-trained model\n",
    "        Following research paper, get the hypercolumn feature until layer 'conv_5_2'\n",
    "    \"\"\"\n",
    "    in_img = preprocess_vgg_means(input_img)\n",
    "#     in_img = input_img\n",
    "    \n",
    "    vgg_18 = _vgg(18)\n",
    "    inp = vgg_18.input\n",
    "    outputs = [layer.output for layer in vgg_18.layers] \n",
    "    functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]\n",
    "    \n",
    "    features = {}\n",
    "    for i in [0, 2, 5, 8, 13, 18]:\n",
    "        features[i] = functors[i]([in_img, 0])[0]\n",
    "        \n",
    "    features_list = []\n",
    "    for i in [0, 2, 5, 8, 13, 18]:\n",
    "        features_list.append(features[i])\n",
    "    return features, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 128, 128, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128, 128, 3), dtype=float32, numpy=\n",
       "array([[[[149.061    , 131.22101  , 121.32     ],\n",
       "         [140.061    , 122.221    , 106.32     ],\n",
       "         [136.061    , 114.221    ,  80.32     ],\n",
       "         ...,\n",
       "         [ 17.060997 ,  -0.7789993, -40.68     ],\n",
       "         [ 25.060997 ,   4.2210007, -39.68     ],\n",
       "         [ 59.060997 ,  39.221    ,  13.32     ]],\n",
       "\n",
       "        [[107.061    ,  80.221    ,  24.32     ],\n",
       "         [105.061    ,  79.221    ,  26.32     ],\n",
       "         [105.061    ,  80.221    ,  31.32     ],\n",
       "         ...,\n",
       "         [ 16.060997 ,  -5.7789993, -46.68     ],\n",
       "         [ 48.060997 ,  31.221    ,  -4.6800003],\n",
       "         [ 69.061    ,  52.221    ,  21.32     ]],\n",
       "\n",
       "        [[ 98.061    ,  78.221    ,  28.32     ],\n",
       "         [105.061    ,  85.221    ,  39.32     ],\n",
       "         [103.061    ,  81.221    ,  36.32     ],\n",
       "         ...,\n",
       "         [140.061    , 122.221    , 100.32     ],\n",
       "         [144.061    , 124.221    , 108.32     ],\n",
       "         [107.061    ,  81.221    ,  48.32     ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[121.061    ,  97.221    ,  58.32     ],\n",
       "         [124.061    , 102.221    ,  58.32     ],\n",
       "         [121.061    ,  99.221    ,  54.32     ],\n",
       "         ...,\n",
       "         [110.061    ,  91.221    ,  55.32     ],\n",
       "         [118.061    ,  99.221    ,  63.32     ],\n",
       "         [114.061    ,  92.221    ,  58.32     ]],\n",
       "\n",
       "        [[116.061    ,  99.221    ,  64.32     ],\n",
       "         [121.061    , 102.221    ,  66.32     ],\n",
       "         [122.061    , 101.221    ,  64.32     ],\n",
       "         ...,\n",
       "         [105.061    ,  84.221    ,  48.32     ],\n",
       "         [110.061    ,  85.221    ,  50.32     ],\n",
       "         [105.061    ,  81.221    ,  46.32     ]],\n",
       "\n",
       "        [[119.061    , 104.221    ,  73.32     ],\n",
       "         [116.061    ,  99.221    ,  64.32     ],\n",
       "         [125.061    , 105.221    ,  71.32     ],\n",
       "         ...,\n",
       "         [101.061    ,  77.221    ,  42.32     ],\n",
       "         [ 94.061    ,  70.221    ,  35.32     ],\n",
       "         [ 98.061    ,  77.221    ,  41.32     ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mean = preprocess_vgg_means(output_image)\n",
    "img_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-85cbd689f72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vgg_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-2c4bc3a20f9a>\u001b[0m in \u001b[0;36mget_vgg_weights\u001b[0;34m(input_img)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-2c4bc3a20f9a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[1;32m   3759\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[0;32m-> 3760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[1;32m   3655\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             base_graph=source_graph)\n\u001b[0m\u001b[1;32m   3658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[0;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "features, features_list = get_vgg_weights(img_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e910b4111290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-e910b4111290>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[1;32m   3759\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[0;32m-> 3760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[1;32m   3655\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             base_graph=source_graph)\n\u001b[0m\u001b[1;32m   3658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[0;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "vgg18 = _vgg(18)\n",
    "\n",
    "inp = vgg18.input\n",
    "outputs = [layer.output for layer in vgg18.layers] \n",
    "functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATOR RIGHT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR (aka model to train for Reflection Removal)\n",
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self.n_filters = 64\n",
    "        self.channels = 3\n",
    "        self.initializer = tf.initializers.he_normal(seed=None)\n",
    "#         self.init_kernel = tf.keras.initializers.Identity(gain= 1.0)\n",
    "\n",
    "#     def get_weight_vgg(self, in_img):\n",
    "#         self.input_img = preprocess_vgg(in_img)\n",
    "#         for block_id in range(1, 6):\n",
    "#             weight = vgg_weight_block(self.input_img, block_id)\n",
    "#             in_img = tf.concat([tf.image.resize(weight, (tf.shape(input= in_img)[1], tf.shape(input= in_img)[2]), \n",
    "#                                                    method=tf.image.ResizeMethod.BILINEAR)/255.0, in_img], axis=3)\n",
    "#         return in_img\n",
    "\n",
    "    def concat_vgg_weight(self, in_img):\n",
    "        self.in_height = tf.shape(input= in_img)[1]\n",
    "        self.in_width = tf.shape(input= in_img)[2] \n",
    "        _, self.weight_list = get_vgg_weights(in_img)\n",
    "        \n",
    "        for block_id in range(1, 6):\n",
    "            weight = self.weight_list[block_id]\n",
    "            weight_resize = tf.image.resize(weight, (self.in_height, self.in_width), \n",
    "                                            method = tf.image.ResizeMethod.BILINEAR) / 255.0\n",
    "            in_img = tf.concat([weight_resize, in_img], axis=3)\n",
    "        return in_img\n",
    "\n",
    "    def activation_normalizer(self, layer):\n",
    "        layer = tf.keras.layers.LeakyReLU(0.2)(layer)\n",
    "        layer = tf.keras.layers.BatchNormalization()(layer)\n",
    "        return layer\n",
    "\n",
    "\n",
    "    def build_gen(self, input_shape):\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape = input_shape)\n",
    "\n",
    "#         vgg19_features = vgg_52(inp) \n",
    "#         vgg19_features = self.get_weight_vgg(inp)\n",
    "        vgg19_features = self.concat_vgg_weight(inp)\n",
    "\n",
    "        gen = tf.keras.layers.Conv2D(filters= 64, kernel_size= [1,1], dilation_rate= 1,padding= 'same', kernel_initializer= self.initializer)(vgg19_features)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 1, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 2, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 4, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 8, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 16, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 32, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 64, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        gen = tf.keras.layers.Conv2D(64, kernel_size= [3,3], dilation_rate= 1, padding= 'same', kernel_initializer= self.initializer)(gen)\n",
    "        gen = self.activation_normalizer(gen)\n",
    "        # last layer\n",
    "        gen = tf.keras.layers.Conv2D(6, kernel_size= [1,1], dilation_rate= 1, padding= 'same')(gen)\n",
    "        gen = Model(inp, gen)\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-41d4a8697953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e273a3dce787>\u001b[0m in \u001b[0;36mbuild_gen\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         vgg19_features = vgg_52(inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         vgg19_features = self.get_weight_vgg(inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mvgg19_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_vgg_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg19_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e273a3dce787>\u001b[0m in \u001b[0;36mconcat_vgg_weight\u001b[0;34m(self, in_img)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0min_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0min_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vgg_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a890b67b0766>\u001b[0m in \u001b[0;36mget_vgg_weights\u001b[0;34m(input_img)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a890b67b0766>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg_18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mfunctors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[1;32m   3759\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[0;32m-> 3760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[1;32m   3655\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             base_graph=source_graph)\n\u001b[0m\u001b[1;32m   3658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[0;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/lift_to_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mvisited_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "generator = Generator().build_gen([256, 256, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_24 (TensorFlowO [(None, 256, 256, 3) 0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_24 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_72 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_24[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_73 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_24[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_74 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_44 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_72[0][0]         \n",
      "                                                                 tf_op_layer_sub_73[0][0]         \n",
      "                                                                 tf_op_layer_sub_74[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_25 (TensorFlowO [(None, 256, 256, 3) 0           tf_op_layer_concat_44[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_25 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_25[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_75 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_25[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_76 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_25[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_77 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_40 (TensorFlo [(4,)]               0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_41 (TensorFlo [(4,)]               0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_45 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_75[0][0]         \n",
      "                                                                 tf_op_layer_sub_76[0][0]         \n",
      "                                                                 tf_op_layer_sub_77[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_40 (T [()]                 0           tf_op_layer_Shape_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_41 (T [()]                 0           tf_op_layer_Shape_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Model)                 multiple             38720       tf_op_layer_concat_45[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_20/size (Ten [(2,)]               0           tf_op_layer_strided_slice_40[0][0\n",
      "                                                                 tf_op_layer_strided_slice_41[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_20/ResizeBil [(None, None, None,  0           model_7[5][0]                    \n",
      "                                                                 tf_op_layer_resize_20/size[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_26 (TensorFlowO [(None, 256, 256, 3) 0           tf_op_layer_concat_44[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_20 (TensorF [(None, None, None,  0           tf_op_layer_resize_20/ResizeBilin\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_26 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_26[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_46 (TensorFl [(None, 256, 256, 67 0           tf_op_layer_truediv_20[0][0]     \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_78 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_26[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_79 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_26[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_80 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_42 (TensorFlo [(4,)]               0           tf_op_layer_concat_46[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_43 (TensorFlo [(4,)]               0           tf_op_layer_concat_46[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_47 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_78[0][0]         \n",
      "                                                                 tf_op_layer_sub_79[0][0]         \n",
      "                                                                 tf_op_layer_sub_80[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_42 (T [()]                 0           tf_op_layer_Shape_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_43 (T [()]                 0           tf_op_layer_Shape_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 multiple             260160      tf_op_layer_concat_47[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_21/size (Ten [(2,)]               0           tf_op_layer_strided_slice_42[0][0\n",
      "                                                                 tf_op_layer_strided_slice_43[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_21/ResizeBil [(None, None, None,  0           model_8[5][0]                    \n",
      "                                                                 tf_op_layer_resize_21/size[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_27 (TensorFlowO [(None, 256, 256, 3) 0           tf_op_layer_concat_44[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_21 (TensorF [(None, None, None,  0           tf_op_layer_resize_21/ResizeBilin\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_27 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_27[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_48 (TensorFl [(None, 256, 256, 19 0           tf_op_layer_truediv_21[0][0]     \n",
      "                                                                 tf_op_layer_concat_46[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_81 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_27[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_82 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_27[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_83 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_44 (TensorFlo [(4,)]               0           tf_op_layer_concat_48[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_45 (TensorFlo [(4,)]               0           tf_op_layer_concat_48[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_49 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_81[0][0]         \n",
      "                                                                 tf_op_layer_sub_82[0][0]         \n",
      "                                                                 tf_op_layer_sub_83[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_44 (T [()]                 0           tf_op_layer_Shape_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_45 (T [()]                 0           tf_op_layer_Shape_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 multiple             1145408     tf_op_layer_concat_49[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_22/size (Ten [(2,)]               0           tf_op_layer_strided_slice_44[0][0\n",
      "                                                                 tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_22/ResizeBil [(None, None, None,  0           model_9[5][0]                    \n",
      "                                                                 tf_op_layer_resize_22/size[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_28 (TensorFlowO [(None, 256, 256, 3) 0           tf_op_layer_concat_44[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_22 (TensorF [(None, None, None,  0           tf_op_layer_resize_22/ResizeBilin\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_28 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_28[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_50 (TensorFl [(None, 256, 256, 45 0           tf_op_layer_truediv_22[0][0]     \n",
      "                                                                 tf_op_layer_concat_48[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_84 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_28[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_85 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_28[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_86 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_46 (TensorFlo [(4,)]               0           tf_op_layer_concat_50[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_47 (TensorFlo [(4,)]               0           tf_op_layer_concat_50[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_51 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_84[0][0]         \n",
      "                                                                 tf_op_layer_sub_85[0][0]         \n",
      "                                                                 tf_op_layer_sub_86[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_46 (T [()]                 0           tf_op_layer_Shape_46[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_47 (T [()]                 0           tf_op_layer_Shape_47[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_10 (Model)                multiple             5865536     tf_op_layer_concat_51[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_23/size (Ten [(2,)]               0           tf_op_layer_strided_slice_46[0][0\n",
      "                                                                 tf_op_layer_strided_slice_47[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_23/ResizeBil [(None, None, None,  0           model_10[5][0]                   \n",
      "                                                                 tf_op_layer_resize_23/size[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_29 (TensorFlowO [(None, 256, 256, 3) 0           tf_op_layer_concat_44[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_23 (TensorF [(None, None, None,  0           tf_op_layer_resize_23/ResizeBilin\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_29 (TensorFlo [(None, 256, 256, 1) 0           tf_op_layer_mul_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_52 (TensorFl [(None, 256, 256, 96 0           tf_op_layer_truediv_23[0][0]     \n",
      "                                                                 tf_op_layer_concat_50[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_87 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_29[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_88 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_29[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_89 (TensorFlowO [(None, 256, 256, 1) 0           tf_op_layer_split_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_48 (TensorFlo [(4,)]               0           tf_op_layer_concat_52[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_49 (TensorFlo [(4,)]               0           tf_op_layer_concat_52[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_53 (TensorFl [(None, 256, 256, 3) 0           tf_op_layer_sub_87[0][0]         \n",
      "                                                                 tf_op_layer_sub_88[0][0]         \n",
      "                                                                 tf_op_layer_sub_89[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_48 (T [()]                 0           tf_op_layer_Shape_48[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_49 (T [()]                 0           tf_op_layer_Shape_49[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_11 (Model)                multiple             15304768    tf_op_layer_concat_53[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_24/size (Ten [(2,)]               0           tf_op_layer_strided_slice_48[0][0\n",
      "                                                                 tf_op_layer_strided_slice_49[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_resize_24/ResizeBil [(None, None, None,  0           model_11[5][0]                   \n",
      "                                                                 tf_op_layer_resize_24/size[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_truediv_24 (TensorF [(None, None, None,  0           tf_op_layer_resize_24/ResizeBilin\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_54 (TensorFl [(None, 256, 256, 14 0           tf_op_layer_truediv_24[0][0]     \n",
      "                                                                 tf_op_layer_concat_52[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 64) 94464       tf_op_layer_concat_54[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 256, 256, 64) 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 256, 64) 256         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 64) 36928       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256, 256, 64) 256         leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 6)  390         batch_normalization_17[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 15,697,350\n",
      "Trainable params: 15,696,198\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()\n",
    "\n",
    "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCRIMINATOR\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        self.filters = 64\n",
    "        self.init_kernel = tf.initializers.he_normal(seed= None)\n",
    "#         self.initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    def lrelu(self, x_in, a):\n",
    "        x = tf.identity(x_in)\n",
    "        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n",
    "    \n",
    "    def conv_block(self, x_in, filters, strides, batch_norm= True, relu_act= True):\n",
    "#         x = tf.keras.layers.Lambda(lambda x: tf.pad(x_in, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\"))\n",
    "        x_in = tf.pad(x_in, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "    \n",
    "        x = tf.keras.layers.Conv2D(filters= filters, kernel_size= 4,\n",
    "                                   strides= strides, use_bias= False, \n",
    "                                   padding=\"VALID\",\n",
    "#                                    padding=\"SAME\", \n",
    "                                   kernel_initializer= self.init_kernel)(x_in)\n",
    "#                                    kernel_initializer= self.initializer)(x_in)\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization()(x)\n",
    "        if relu_act:\n",
    "            x = self.lrelu(x, 0.2)\n",
    "#             x = LeakyReLU(0.2)(x)\n",
    "        return x\n",
    "\n",
    "    # with build(input_shape):\n",
    "    def build_discriminator(self, input_shape):\n",
    "        inp = tf.keras.layers.Input(shape=input_shape, name='input_image')\n",
    "        tar = tf.keras.layers.Input(shape=input_shape, name='target_image')\n",
    "\n",
    "        x_in = tf.keras.layers.concatenate([inp, tar])\n",
    "    \n",
    "#    # with build(discrim_inputs, discrim_targets):\n",
    "#     def build_discriminator(self, discrim_inputs, discrim_targets):\n",
    "#         input_shape = tf.keras.layers.concatenate([discrim_inputs, discrim_targets]).shape\n",
    "        \n",
    "#         x_in = tf.keras.layers.Input(shape= input_shape)\n",
    "        \n",
    "    # Discriminator Layers:     \n",
    "        x = self.conv_block(x_in, self.filters, 2, False)  #Layer 1, filters = 64        \n",
    "        \n",
    "        x = self.conv_block(x, self.filters *2, 2) #Layer 2, filters = 128\n",
    "        x = self.conv_block(x, self.filters *4, 2) #Layer 3, filters = 256\n",
    "        x = self.conv_block(x, self.filters *8, 1) #Layer 4, filters = 512  \n",
    "        x = self.conv_block(x, 1, 1, False, False) #Layer 5\n",
    "        \n",
    "        x = tf.keras.activations.sigmoid(x)\n",
    "#         x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "#         x = Model(x_in, x)\n",
    "        x = Model(inputs= [inp, tar], outputs= x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().build_discriminator(input_shape= (IMG_WIDTH, IMG_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Loss():\n",
    "#     def _perceptual_loss(self,):\n",
    "\n",
    "# Loss functions\n",
    "# #L1 loss\n",
    "def compute_l1_loss(in_img, out_img):\n",
    "    return tf.reduce_mean(tf.abs(in_img - out_img))\n",
    "\n",
    "def _l1_loss_reflection(gen_refl, refl, synthetic= False):\n",
    "    loss_l1_r = compute_l1_loss(gen_refl, refl) if synthetic else 0\n",
    "    return loss_l1_r\n",
    "\n",
    "# #Perceptual loss\n",
    "def _perceptual_loss(gen_img, target_img):\n",
    "    _, vgg_feature_fake = get_vgg_weights(gen_img)\n",
    "    _, vgg_feature_real = get_vgg_weights(target_img)\n",
    "    \n",
    "    # compute_l1_loss(real, fake)\n",
    "    p0 = compute_l1_loss(vgg_feature_real[0], vgg_feature_fake[0])             # Input, layer 0\n",
    "    p1 = compute_l1_loss(vgg_feature_real[1], vgg_feature_fake[1]) /2.6        # conv1_2, layer 2   \n",
    "    p2 = compute_l1_loss(vgg_feature_real[2], vgg_feature_fake[2]) /4.8        # conv2_2, layer 5 \n",
    "    p3 = compute_l1_loss(vgg_feature_real[3], vgg_feature_fake[3]) /3.7        # conv3_2, layer 8 \n",
    "    p4 = compute_l1_loss(vgg_feature_real[4], vgg_feature_fake[4]) /5.6      # conv4_2, layer 13 \n",
    "    p5 = compute_l1_loss(vgg_feature_real[5], vgg_feature_fake[5]) *10/1.5   # conv5_2, layer 18 \n",
    "\n",
    "    return p0+p1+p2+p3+p4+p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusion loss, in gradient domain  #FLAG, not using synthetic data so do we need this?\n",
    "def compute_gradient(img):\n",
    "    gradx = img[:,1:,:,:] - img[:,:-1,:,:]\n",
    "    grady = img[:,:,1:,:] - img[:,:,:-1,:]\n",
    "    return gradx, grady\n",
    "\n",
    "# img1: transmission layer, img2: reflection layer\n",
    "def _exclusion_loss(img1, img2, level=1):\n",
    "    gradx_loss=[]\n",
    "    grady_loss=[]\n",
    "    \n",
    "#     trans_layer = img1\n",
    "#     refl_layer = img2\n",
    "    \n",
    "    for l in range(level):\n",
    "        gradx1, grady1 = compute_gradient(img1)\n",
    "        gradx2, grady2 = compute_gradient(img2)\n",
    "        alphax = 2.0* tf.reduce_mean(tf.abs(gradx1)) / tf.reduce_mean(tf.abs(gradx2))\n",
    "        alphay = 2.0* tf.reduce_mean(tf.abs(grady1)) / tf.reduce_mean(tf.abs(grady2))\n",
    "        \n",
    "        gradx1_s = (tf.keras.activations.sigmoid(gradx1) *2) -1\n",
    "        grady1_s = (tf.keras.activations.sigmoid(grady1) *2) -1\n",
    "        gradx2_s = (tf.keras.activations.sigmoid(gradx2 *alphax) *2) -1\n",
    "        grady2_s = (tf.keras.activations.sigmoid(grady2 *alphay) *2) -1\n",
    "\n",
    "        gradx_loss.append( tf.reduce_mean( tf.multiply( tf.square(gradx1_s), tf.square(gradx2_s)) ) **0.25)\n",
    "        grady_loss.append( tf.reduce_mean( tf.multiply( tf.square(grady1_s), tf.square(grady2_s)) ) **0.25)\n",
    "\n",
    "#         img1= tf.nn.avg_pool(img1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "#         img2= tf.nn.avg_pool(img2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        img1= tf.keras.layers.AveragePooling2D( pool_size= (2,2), padding='SAME')(img1)\n",
    "        img2= tf.keras.layers.AveragePooling2D( pool_size= (2,2), padding='SAME')(img2)\n",
    "\n",
    "    return gradx_loss, grady_loss\n",
    "\n",
    "def _gradient_loss(gen_img, refl_img, synthetic= False):\n",
    "    loss_gradx, loss_grady = _exclusion_loss(gen_img, refl_img, level=3)\n",
    "    loss_gradxy = tf.reduce_sum(sum(loss_gradx) /3.) + tf.reduce_sum(sum(loss_grady) /3.)\n",
    "    \n",
    "    # loss_grad = tf.where(issyn, loss_gradxy/2.0, 0) If is synthetic, = loss_gradxy/2, else 0\n",
    "    # loss_grad = loss_gradxy / 2.0\n",
    "    loss_grad = loss_gradxy / 2.0 if synthetic else 0\n",
    "    return loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adversarial Loss (aka Generator loss & Discriminator loss)\n",
    "# EPS = 1e-12\n",
    "def _adversarial_loss(disc_real_out, disc_gen_out):\n",
    "#     predict_real = disc_real_out\n",
    "#     predict_fake = disc_gen_out\n",
    "#     # Compute Gen & Disc loss\n",
    "#     gen_loss = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "#     disc_loss = (tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))) * 0.5\n",
    "    \n",
    "    gen_loss = tf.reduce_mean(-tf.log(disc_gen_out + EPS))\n",
    "    disc_loss = (tf.reduce_mean(-(tf.log(disc_real_out + EPS) + tf.log(1 - disc_gen_out + EPS)))) * 0.5\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 loss on reflection image\n",
    "## loss_l1_r = tf.where(issyn, compute_l1_loss(reflection_layer, reflection), 0)\n",
    "# loss_l1_r = compute_l1_loss(reflection_layer, reflection)\n",
    "loss_l1_r = _l1_loss_reflection(gen_refl, refl)\n",
    "\n",
    "# Perceptual Loss  #FLAG, not using synthetic data so reflection loss is pretty useless here\n",
    "loss_percep_t = _perceptual_loss(transmission_layer, target)\n",
    "# loss_percep_r = tf.where(issyn, compute_percep_loss(reflection_layer, reflection, reuse=True), 0.)\n",
    "loss_percep_r = _perceptual_loss(reflection_layer, reflection)\n",
    "# loss_percep = tf.where(issyn, loss_percep_t+loss_percep_r, loss_percep_t)\n",
    "loss_percep = (loss_percep_t + loss_percep_r) if synthetic else loss_percep_t\n",
    "# FOR REAL IMAGES ONLY\n",
    "loss_percep = loss_percep_t \n",
    "\n",
    "\n",
    "# Exclusion loss\n",
    "loss_grad = _gradient_loss(gen_img, refl_img)\n",
    "\n",
    "# TOTAL LOSS\n",
    "loss = loss_l1_r + loss_percep *0.2 + loss_grad\n",
    "\n",
    "# OR IF YOU ONLY USE REAL IMAGES FOR TRAINING\n",
    "# generator_loss += content_loss + perceptual_loss\n",
    "# discriminator_loss = self.loss._discriminator_loss(hr_output, sr_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "  \n",
    " #      def pretrain_optimizer(self):\n",
    "#         learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(PRETRAIN_LR, \n",
    "#                                                                        PRETRAIN_LR_DECAY_STEP, \n",
    "#                                                                        0.5, staircase=True)\n",
    "#         pre_gen_optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "#         return pre_gen_optimizer\n",
    "\n",
    "    def gan_optimizer(self):\n",
    "        boundaries = [50000, 100000, 200000, 300000]\n",
    "        values = [GAN_LR, GAN_LR * 0.5, GAN_LR * 0.5 ** 2,\n",
    "                  GAN_LR * 0.5 ** 3, GAN_LR * 0.5 ** 4]\n",
    "        learning_rate = PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "        dis_optimizer = Adam(learning_rate=learning_rate)\n",
    "        gen_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        return dis_optimizer, gen_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up optimizer\n",
    "generator_optimizer, discriminator_optimizer = Optimizer().gan_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG, optimizers from paper\n",
    "\n",
    "# train_vars = tf.compat.v1.trainable_variables()\n",
    "# d_vars = [var for var in train_vars if 'discriminator' in var.name]\n",
    "# g_vars = [var for var in train_vars if 'g_' in var.name]\n",
    "# g_opt=tf.compat.v1.train.AdamOptimizer(learning_rate=0.0002).minimize(loss*100+g_loss, var_list=g_vars) # optimizer for the generator\n",
    "# d_opt=tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(d_loss,var_list=d_vars) # optimizer for the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF is_training\n",
    "def prepare_data(train_path):\n",
    "    input_names=[]\n",
    "    image1=[]\n",
    "    image2=[]\n",
    "    for dirname in train_path:\n",
    "        train_t_gt = dirname + \"transmission_layer/\"\n",
    "        train_r_gt = dirname + \"reflection_layer/\"\n",
    "        train_b = dirname + \"blended/\"\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(train_t_gt)):\n",
    "            for fname in fnames:\n",
    "                if is_image_file(fname):\n",
    "                    path_input = os.path.join(train_b, fname)\n",
    "                    path_output1 = os.path.join(train_t_gt, fname)\n",
    "                    path_output2 = os.path.join(train_r_gt, fname)\n",
    "                    \n",
    "                    input_names.append(path_input)\n",
    "                    image1.append(path_output1)\n",
    "                    image2.append(path_output2)\n",
    "    return input_names, image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8ca9cfcbd1c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m summary_writer = tf.summary.create_file_writer(\n\u001b[0;32m----> 5\u001b[0;31m   log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "log_dir = LOG_DIR\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().build_gen(input_shape=(None, None, 3))\n",
    "discriminator = Discriminator().build_discriminator(input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)  #FLAG, OUTPUT of GENERATOR IS (None, None, None, 6)\n",
    "        gen_output_transmission, gen_output_reflection = tf.split(gen_output, num_or_size_splits=2, axis=3) \n",
    "    \n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output_transmission], training=True)\n",
    "    \n",
    "#         gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "#         disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        gen_loss, disc_loss = _adversarial_loss(disc_real_output, disc_generated_output)\n",
    "        perceptual_loss = _perceptual_loss(gen_output, target)\n",
    "        \n",
    "        loss = perceptual_loss *0.2 #+ loss_l1_r +loss_grad\n",
    "        gen_total_loss = gen_loss + loss*100 #FLAG, 100 or 100.0\n",
    "  \n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                            generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                                 discriminator.trainable_variables)\n",
    "  \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                            generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                                discriminator.trainable_variables))\n",
    "  \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for example_input, example_target in test_ds.take(1):\n",
    "            generate_images(generator, example_input, example_target)\n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "    # Train\n",
    "    for n, (input_image, target) in train_ds.enumerate():\n",
    "        print('.', end='')\n",
    "        if (n+1) % 100 == 0:\n",
    "            print()\n",
    "        train_step(input_image, target, epoch)\n",
    "    print()\n",
    "\n",
    "    # saving (checkpoint) the model every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
